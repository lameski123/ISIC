{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lameski/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/users/lameski/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/users/lameski/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/users/lameski/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/users/lameski/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/users/lameski/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#%load_ext tensorboard\n",
    "import numpy as np\n",
    "#!pip install torch torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.asarray([[0,1],[1,1],[4,0],[5,0],[6,0],[7,0],[8,1],[9,0],[10,0]])\n",
    "# X_ = X.T\n",
    "# X_train, y_train = train_test_split(X, test_size=0.33,stratify = X_[1])\n",
    "# print(X_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX TITAN X'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = getpass.getuser()\n",
    "\n",
    "train_path = f'/home/users/{username}/kaggle-data/jpeg/train'\n",
    "train_meta = f'/home/users/{username}/kaggle-data/train.csv'\n",
    "\n",
    "test_path = f'/home/users/{username}/kaggle-data/jpeg/test'\n",
    "test_meta = f'/home/users/{username}/kaggle-data/test.csv'\n",
    "\n",
    "\n",
    "\n",
    "classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_path, meta_path, test_path, test_meta_path,tr = True, test= False):\n",
    "        super(KaggleDataset, self).__init__()\n",
    "        self.img_path = img_path\n",
    "        self.meta_path = meta_path\n",
    "        self.meta = pd.read_csv(self.meta_path)\n",
    "        self.tr = tr\n",
    "        self.test = test\n",
    "        self.train, self.val = train_test_split(self.meta, test_size = 0.2, stratify = self.meta.target)\n",
    "        if test:\n",
    "            self.test_path = test_path\n",
    "            self.test_meta_path = test_meta_path\n",
    "            self.test_meta = pd.read_csv(self.test_meta_path)\n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.tr and not self.test:\n",
    "            return len(self.train)\n",
    "        elif not self.tr and not self.test:\n",
    "            return len(self.val)\n",
    "        else: \n",
    "            return len(self.test_meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.tr and not self.test:\n",
    "            label = self.train.iloc()[idx]\n",
    "\n",
    "            target = label.target\n",
    "\n",
    "            path = os.path.join(self.img_path, label.image_name + \".jpg\")\n",
    "            input_image = Image.open(path)\n",
    "            preprocess = T.Compose([\n",
    "                T.Resize((224,224)),\n",
    "                #T.CenterCrop(224),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "            input_tensor = preprocess(input_image)\n",
    "            #input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "            if input_tensor.shape[0]==1:\n",
    "                input_tensor = input_tensor.repeat(3,1,1)\n",
    "            return input_tensor, target\n",
    "        elif not self.tr and not self.test:\n",
    "            label = self.val.iloc()[idx]\n",
    "\n",
    "            target = label.target\n",
    "\n",
    "            path = os.path.join(self.img_path, label.image_name + \".jpg\")\n",
    "            input_image = Image.open(path)\n",
    "            preprocess = T.Compose([\n",
    "                T.Resize((224,224)),\n",
    "                #T.CenterCrop(224),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "            input_tensor = preprocess(input_image)\n",
    "            #input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "            if input_tensor.shape[0]==1:\n",
    "                input_tensor = input_tensor.repeat(3,1,1)\n",
    "            return input_tensor, target\n",
    "        \n",
    "        else:\n",
    "            label = self.test_meta.iloc()[idx]\n",
    "\n",
    "#             target = self.label.index(meta.target)\n",
    "\n",
    "            path = os.path.join(self.test_path, label.image_name + \".jpg\")\n",
    "            input_image = Image.open(path)\n",
    "            preprocess = T.Compose([\n",
    "                T.Resize((224,224)),\n",
    "                #T.CenterCrop(224),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "            input_tensor = preprocess(input_image)\n",
    "            #input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "            if input_tensor.shape[0]==1:\n",
    "                input_tensor = input_tensor.repeat(3,1,1)\n",
    "            \n",
    "            return input_tensor, label.image_name#self.test_meta['image_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.train, self.val = train_test_split(meta, test_size = 0.2, stratify = meta.target)\n",
    "def get_dataset(img_path, meta_path,test_path, test_meta_path, train, test):\n",
    "    dataset = KaggleDataset(img_path, meta_path,test_path, test_meta_path, train, test)\n",
    "    return dataset\n",
    "\n",
    "def get_loader(dataset, shuffle, size=128):\n",
    "    loader = DataLoader(dataset, batch_size=size,\n",
    "                        shuffle=shuffle, num_workers=8)\n",
    "    return loader\n",
    "\n",
    "def build_dataloader(img_path, meta_path,test_path, test_meta_path,):\n",
    " \n",
    "    train_dataset = get_dataset(img_path, meta_path,test_path, test_meta_path,\n",
    "                                True, False)\n",
    "    val_dataset = get_dataset(img_path, meta_path,test_path, test_meta_path,\n",
    "                              False, False)\n",
    "    test_dataset = get_dataset(img_path, meta_path,test_path, test_meta_path,\n",
    "                               True, True)\n",
    "    \n",
    "    train_loader = get_loader(train_dataset, True)\n",
    "    val_loader = get_loader(val_dataset, False)\n",
    "    test_loader = get_loader(test_dataset, False)\n",
    "    \n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet.fc\n",
    "# Linear(in_features=2048, out_features=1000, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(pl.LightningModule):\n",
    "    \n",
    "\n",
    "    def __init__(self, freeze=True, lr=0.01, names=None):\n",
    "        super(Resnet, self).__init__()\n",
    "        self.resnet = models.resnext50_32x4d(pretrained=True)\n",
    "        \n",
    "        if freeze is True:\n",
    "            # freeze feature layers\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # adjust classes\n",
    "        self.resnet.fc = nn.Linear(2048, classes)\n",
    "        \n",
    "        self.loaders = None\n",
    "        self.lr = lr\n",
    "        self.names = names\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        return self.resnet(inp)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        tensorboard_logs = {'train_loss' : loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = SGD(self.parameters(),\n",
    "                       lr=self.lr,\n",
    "                       momentum=0.9,\n",
    "                       weight_decay=0.0005)\n",
    "        scheduler = StepLR(optimizer, 1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        return {'val_loss': loss}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "#     def test_step(self, batch, batch_nb):\n",
    "#         x, y = batch\n",
    "#         y_hat = self(x).argmax(dim=1)\n",
    "#         acc = (y == y_hat).float().mean()\n",
    "#         c_accs = []\n",
    "#         for i in range(classes):\n",
    "#             class_elem = y == i\n",
    "#             c_acc = (y_hat[class_elem] == i).sum().float() / (class_elem.sum() + 1e-9)\n",
    "#             c_accs.append(c_acc)\n",
    "#         return {'accuracy' : acc, 'class_accuracies' : c_accs}\n",
    "#         return y_hat\n",
    "\n",
    "#     def test_epoch_end(self, outputs):\n",
    "#         avg_acc = torch.stack([x['accuracy'] for x in outputs]).mean().item()\n",
    "#         c_accs = [x['class_accuracies'] for x in outputs]\n",
    "#         avg_c_acc = [torch.stack([l[i] for l in c_accs]).mean().item() for i in range(classes)]\n",
    "#         if self.names is not None:\n",
    "#             names = pd.read_pickle(self.names)\n",
    "#             names = names[0].to_list()\n",
    "#             avg_c_acc = list(zip(names, avg_c_acc))\n",
    "#         return {'accuracy' : avg_acc, 'class_accuracies': avg_c_acc}\n",
    "    \n",
    "    def build_loaders(self):\n",
    "        self.loaders = \\\n",
    "            build_dataloader(train_path, train_meta, test_path, test_meta)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        if self.loaders is None:\n",
    "            self.build_loaders()\n",
    "        return self.loaders[0]\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        if self.loaders is None:\n",
    "            self.build_loaders()\n",
    "        return self.loaders[1]\n",
    "    \n",
    "    def test_data(self):\n",
    "        if self.loaders is None:\n",
    "            self.build_loaders()\n",
    "        return self.loaders[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(version='v1'):\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath='models/resnext50_32x4d.ckpt',\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer( \n",
    "        checkpoint_callback=checkpoint_callback,\n",
    "        gpus=[0],\n",
    "        distributed_backend='dp', \n",
    "        max_nb_epochs=3,\n",
    "        val_check_interval=0.25,\n",
    "    )\n",
    "    \n",
    "    if version == 'v1':\n",
    "        model = Resnet()\n",
    "    else:\n",
    "        model = Resnet(freeze=False, lr=0.001)\n",
    "        model.load_state_dict(torch.load('checkpoints/resnext50_32x4dv1.pt'))\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "    torch.save(model.state_dict(), f'checkpoints/resnext50_32x4d{version}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if version == 'v1':\n",
    "# #         model = Resnet()\n",
    "# #     else:\n",
    "# # checkpoint_callback = ModelCheckpoint(\n",
    "# #         filepath='models/_ckpt_epoch_2.ckpt',\n",
    "# #         verbose=True,\n",
    "# #     )\n",
    "# #model = Resnet()#.load_from_checkpoint(checkpoint_path=\"models/_ckpt_epoch_2.ckpt\")\n",
    "# # trainer = pl.Trainer( \n",
    "# #     checkpoint_callback=checkpoint_callback,\n",
    "# #     gpus=[0],\n",
    "# #     distributed_backend='dp', \n",
    "# #     max_nb_epochs=1,\n",
    "# #     val_check_interval=0.25,\n",
    "# # )\n",
    "# trainer = pl.Trainer(resume_from_checkpoint='models/_ckpt_epoch_2.ckpt',\n",
    "#                         gpus=[0],\n",
    "#                         distributed_backend='dp', \n",
    "#                         max_nb_epochs=1,\n",
    "#                         val_check_interval=0.25)\n",
    "# #model = Resnet(freeze=False, lr=0.001)\n",
    "# #model.load_state_dict(torch.load('models/resnext50_32x4dv1.pt'))\n",
    "# model = Resnet()\n",
    "# trainer.fit(model)\n",
    "\n",
    "# torch.save(model.state_dict(), 'models/resnext50_32x4dv1.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File \"isic_notebook.py\", line 309, in run_training\n",
    "    torch.save(model.state_dict(), f'checkpoints/resnext50_32x4d{version}.pt')\n",
    "  File \"/home/users/lameski/.local/lib/python3.6/site-packages/torch/serialization.py\", line 369, in save\n",
    "    with _open_file_like(f, 'wb') as opened_file:\n",
    "  File \"/home/users/lameski/.local/lib/python3.6/site-packages/torch/serialization.py\", line 234, in _open_file_like\n",
    "    return _open_file(name_or_buffer, mode)\n",
    "  File \"/home/users/lameski/.local/lib/python3.6/site-packages/torch/serialization.py\", line 215, in __init__\n",
    "    super(_open_file, self).__init__(open(name, mode))\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'checkpoints/resnext50_32x4dv1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=[0],\n",
    "        distributed_backend='dp')\n",
    "    model = Resnet()\n",
    "    model.load_state_dict(torch.load('checkpoints/resnext50_32x4dv1.pt'))\n",
    "    inp, names = model.test_data()\n",
    "    model.eval()\n",
    "    output = model(inp).argmax(dim=1)\n",
    "    \n",
    "    with open('output.txt', 'w') as file:\n",
    "        file.write(str(output))\n",
    "        \n",
    "    frame = zip(names, output)\n",
    "    with open('test_output.txt', 'w') as file:\n",
    "        file.write(str(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    device = torch.device('cuda:0')\n",
    "    cpu = torch.device('cpu')\n",
    "    model = Resnet('/models/resnext50_32x4d.pt')\n",
    "    model.to(device)\n",
    "    _,_,loader = build_dataloader(train_path, train_meta, test_path, test_meta)\n",
    "    #loader = model.test_data()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, name in tqdm(loader):\n",
    "            img = img.to(device)\n",
    "            pred = model(img).argmax(dim=1)\n",
    "            preds.append([name,pred.to(cpu)])\n",
    "\n",
    "    #data = [preds[i] for i in range(len(preds))]\n",
    "    df = pd.DataFrame(preds, columns=['image_name', 'target'])\n",
    "    df.to_csv('submission.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [11:14<00:00,  7.84s/it] \n"
     ]
    }
   ],
   "source": [
    "#run_training()\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "df = pd.read_csv(\"submission.csv\")\n",
    "# num1 = df.target.sum()\n",
    "# overall = len(df.target)\n",
    "# print(num1, overall)\n",
    "names = []\n",
    "targets = []\n",
    "# for n in df.image_name:\n",
    "#     #for nn in n:\n",
    "#     names.append(n)\n",
    "# for t in df.target:\n",
    "#     targets.append(t)\n",
    "#print(re.split(', |,\\n        ',df.target[0][8:-2]))#.split(\", \"))\n",
    "for i in range(len(df)):\n",
    "    temp = df.image_name[i][2:-2].split(\"', '\")\n",
    "    for n in temp:\n",
    "        names.append(n)\n",
    "    temp = re.split(', |,\\n        ',df.target[i][8:-2])\n",
    "    for t in temp:\n",
    "        targets.append(t)\n",
    "\n",
    "targets = [0 if i=='1' else 1 for i in targets]\n",
    "#print(names, )\n",
    "output = pd.DataFrame(list(zip(names, targets)), columns=['image_name', 'target'])\n",
    "output.to_csv(\"submission_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
